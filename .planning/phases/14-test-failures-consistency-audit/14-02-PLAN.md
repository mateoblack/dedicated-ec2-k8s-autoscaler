---
phase: 14-test-failures-consistency-audit
plan: 02
type: execute
---

<objective>
Audit codebase for consistency issues between implementation and tests.

Purpose: Identify gaps where tests don't match implementation, preventing future test failures.
Output: Documented audit findings with any additional test fixes applied.
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-phase.md
./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/14-test-failures-consistency-audit/14-01-SUMMARY.md

**Audit focus areas:**
- Test assertions that check for specific strings in generated code
- Log message assertions that may drift from actual implementation
- Pattern-based checks that could be brittle

@.planning/codebase/TESTING.md
@test/lambda-code-generators.test.ts
@test/token-management.test.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Audit Lambda code generator tests for brittle assertions</name>
  <files>test/lambda-code-generators.test.ts</files>
  <action>
Review all tests in lambda-code-generators.test.ts that use `toContain()` on generated code:

1. Check for exact string matches that could break if implementation changes
2. Identify assertions that test implementation details vs. behavior
3. Look for patterns similar to the setup_logging issue - hardcoded function signatures

Document any brittle assertions found. Fix only clear issues that match the established pattern (partial string matching for flexibility).

WHY: Avoid hardcoding exact function signatures in tests - prefer matching the function name/pattern without the full signature when the signature may evolve.
  </action>
  <verify>grep -n "toContain\|toMatch" test/lambda-code-generators.test.ts | wc -l</verify>
  <done>Audit complete, any brittle assertions identified and fixed or documented</done>
</task>

<task type="auto">
  <name>Task 2: Audit bootstrap script tests for message consistency</name>
  <files>test/token-management.test.ts, test/worker-node-bootstrap.test.ts, test/control-plane-join.test.ts</files>
  <action>
Review tests that check for specific log messages in bootstrap scripts:

1. Cross-reference test assertions with actual log messages in lib/scripts/
2. Identify any assertions checking for old message formats vs. structured logging format
3. Look for hardcoded error messages that may have changed with log_error format

Fix any mismatches found. The structured logging format uses:
- log_info "message" "key=value"
- log_error "message" "check=..." "common_causes=..."

Tests should match the actual message patterns.
  </action>
  <verify>npm run test:code -- test/token-management.test.ts test/worker-node-bootstrap.test.ts test/control-plane-join.test.ts 2>&1 | tail -3</verify>
  <done>All audited tests pass, any message mismatches fixed</done>
</task>

<task type="auto">
  <name>Task 3: Final verification and document findings</name>
  <files>-</files>
  <action>
1. Run full test suite one final time
2. Document audit findings:
   - Number of tests reviewed
   - Issues found and fixed
   - Patterns identified for future test maintenance
3. Ensure all tests pass
  </action>
  <verify>npm run test:code 2>&1 | grep -E "Test Suites:|Tests:"</verify>
  <done>Full test suite passes, audit documented in SUMMARY.md</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run test:code` passes with 0 failures
- [ ] All lambda-code-generators tests pass
- [ ] All bootstrap script tests pass
- [ ] Audit findings documented
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No test failures
- Audit complete with findings documented
- Phase 14 complete, ready for Phase 15
</success_criteria>

<output>
After completion, create `.planning/phases/14-test-failures-consistency-audit/14-02-SUMMARY.md`
</output>
