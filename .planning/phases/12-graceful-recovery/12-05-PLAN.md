---
phase: 12-graceful-recovery
plan: 05
type: execute
---

<objective>
Emit retry metrics to CloudWatch for operational visibility into retry behavior.

Purpose: Understanding retry patterns reveals API throttling, network issues, and service degradation before they cause failures. Metrics enable alerting on elevated retry rates.
Output: Retry attempt and failure metrics emitted from both bash and Python retry functions, integrated with existing metrics infrastructure from Phase 10.
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-phase.md
./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plans in this phase:
@.planning/phases/12-graceful-recovery/12-04-PLAN.md

# Phase 10 context (metrics infrastructure):
@.planning/phases/10-cloudwatch-metrics/10-02-SUMMARY.md

# Key files:
@lib/scripts/bash-retry.ts
@lib/scripts/bash-metrics.ts
@lib/scripts/python-retry.ts
@lib/scripts/python-metrics.ts

**Tech stack available:** TypeScript, Bash, Python 3.11, CloudWatch EMF, Jest
**Established patterns:**
- Bash metrics via emit_metric function (from 10-02)
- Python metrics via MetricsLogger class with EMF (from 10-01)
**Constraining decisions:**
- Phase 10: Metrics namespace is K8sCluster/{SubNamespace}
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add retry metrics to bash retry functions</name>
  <files>lib/scripts/bash-retry.ts</files>
  <action>
Add metrics emission to retry_command() and retry_command_timeout() functions.

Metrics to emit (using emit_metric if available):
1. RetryAttempt: Count metric, emitted on each retry (not first attempt)
2. RetryExhausted: Count metric, emitted when all retries fail

Implementation:
1. Check if emit_metric function exists (Phase 10 may not be deployed everywhere)
2. On retry (attempt > 1): emit_metric "RetryAttempt" 1 "Count"
3. On exhaustion: emit_metric "RetryExhausted" 1 "Count"
4. Use conditional: `command -v emit_metric >/dev/null 2>&1 && emit_metric ...`

Note: retry_command_output doesn't log, so only add to retry_command and retry_command_timeout.
  </action>
  <verify>Grep for "emit_metric.*RetryAttempt" in bash-retry.ts</verify>
  <done>retry_command emits RetryAttempt and RetryExhausted metrics when emit_metric is available</done>
</task>

<task type="auto">
  <name>Task 2: Add retry metrics to Python retry function</name>
  <files>lib/scripts/python-retry.ts</files>
  <action>
Add metrics emission to retry_with_backoff() function.

Since Lambda functions have metrics logger (Phase 10), add optional metrics_logger parameter:

```python
def retry_with_backoff(
    operation,
    operation_name,
    max_retries=3,
    base_delay=5,
    jitter_factor=0.3,
    retriable_exceptions=(Exception,),
    metrics_logger=None  # Optional MetricsLogger instance
):
```

Metrics to emit:
1. RetryAttempt: On each retry (attempt > 1), dimension operation_name
2. RetryExhausted: When all retries fail, dimension operation_name

Implementation:
```python
if metrics_logger and attempt > 1:
    metrics_logger.add_metric('RetryAttempt', 1, 'Count')
    metrics_logger.add_dimension('Operation', operation_name)
    metrics_logger.flush()
```

On exhaustion:
```python
if metrics_logger:
    metrics_logger.add_metric('RetryExhausted', 1, 'Count')
    metrics_logger.add_dimension('Operation', operation_name)
    metrics_logger.flush()
```
  </action>
  <verify>Grep for "RetryAttempt" and "RetryExhausted" in python-retry.ts</verify>
  <done>retry_with_backoff emits retry metrics via optional metrics_logger parameter</done>
</task>

<task type="auto">
  <name>Task 3: Add retry metrics tests</name>
  <files>test/scripts/bash-retry.test.ts, test/scripts/python-retry.test.ts</files>
  <action>
Add tests for retry metrics:

In bash-retry.test.ts:
1. Test RetryAttempt metric emission is present
2. Test RetryExhausted metric emission is present
3. Test emit_metric check uses command -v

In python-retry.test.ts:
1. Test metrics_logger parameter exists in function signature
2. Test RetryAttempt metric name is used
3. Test RetryExhausted metric name is used
  </action>
  <verify>Run `npm run test:code -- bash-retry python-retry` to verify tests pass</verify>
  <done>Tests verify retry metrics are emitted from both bash and Python functions</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run test:code` passes all tests
- [ ] Bash retry_command emits RetryAttempt and RetryExhausted metrics
- [ ] Python retry_with_backoff accepts metrics_logger parameter
- [ ] Python emits metrics with operation_name dimension
- [ ] Metrics are optional (no errors if emit_metric/metrics_logger unavailable)
- [ ] No TypeScript errors
</verification>

<success_criteria>

- Retry metrics emitted from both bash and Python retry functions
- RetryAttempt: count of retry attempts (excludes first try)
- RetryExhausted: count of complete failures after all retries
- Metrics are optional/conditional (graceful if not available)
- Tests verify implementation
- All existing tests pass
- Phase 12 (Graceful Recovery) complete
</success_criteria>

<output>
After completion, create `.planning/phases/12-graceful-recovery/12-05-SUMMARY.md`
</output>
