---
phase: 10-cloudwatch-metrics
plan: 01
type: execute
---

<objective>
Create Python EMF (Embedded Metric Format) module for Lambda functions to emit custom CloudWatch metrics.

Purpose: Enable Lambda functions to emit business-level metrics (backup success, member removal, health checks) without direct PutMetricData API calls - CloudWatch extracts metrics from structured logs automatically.
Output: `lib/scripts/python-metrics.ts` module with `getPythonMetricsSetup()` function, exported from `lib/scripts/index.ts`, with comprehensive unit tests.
</objective>

<execution_context>
./.claude/get-shit-done/workflows/execute-phase.md
./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase context (same pattern we're following):
@.planning/phases/09-structured-logging/09-03-SUMMARY.md
@lib/scripts/python-logging.ts

# EMF specification reference:
# - _aws.CloudWatchMetrics array contains namespace, dimensions, metrics
# - Metrics are numeric values at root level, referenced by name in MetricDefinition
# - Maximum 100 metrics per EMF document
# - Dimensions are key-value pairs, max 30 per dimension set
# - Timestamp in milliseconds since epoch
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create python-metrics.ts module with EMF helper</name>
  <files>lib/scripts/python-metrics.ts</files>
  <action>
Create a new module following the python-logging.ts pattern. The module exports `getPythonMetricsSetup()` which returns Python code string containing:

1. `MetricsLogger` class that:
   - Takes `namespace` (string) and `default_dimensions` (dict) in constructor
   - Has `put_metric(name, value, unit='Count')` method to queue a metric
   - Has `set_dimension(key, value)` method for per-metric dimensions
   - Has `flush()` method that outputs EMF JSON to stdout and clears queue
   - Stores metrics in internal list, flushes automatically at 100 metrics (EMF limit)
   - Uses `_aws` structure per EMF spec: `{"_aws": {"Timestamp": ms, "CloudWatchMetrics": [{"Namespace": "...", "Dimensions": [[...]], "Metrics": [...]}]}, ...values...}`

2. `create_metrics_logger(namespace, context=None)` factory function that:
   - Creates MetricsLogger with namespace
   - Sets default dimensions: `ClusterName` from `CLUSTER_NAME` env var, `FunctionName` from Lambda context if available
   - Returns configured MetricsLogger

Unit constants for common metric units: `COUNT = 'Count'`, `MILLISECONDS = 'Milliseconds'`, `BYTES = 'Bytes'`, `SECONDS = 'Seconds'`.

IMPORTANT: Do NOT add external dependencies (like aws-embedded-metrics) - emit raw EMF JSON directly to stdout. This keeps Lambda code self-contained as per project constraints.
  </action>
  <verify>File exists at lib/scripts/python-metrics.ts with getPythonMetricsSetup() export</verify>
  <done>Module contains MetricsLogger class with put_metric/flush methods, EMF JSON structure correct per spec</done>
</task>

<task type="auto">
  <name>Task 2: Export from scripts module and add unit tests</name>
  <files>lib/scripts/index.ts, test/scripts/python-metrics.test.ts</files>
  <action>
1. Update lib/scripts/index.ts to export `getPythonMetricsSetup` from './python-metrics'

2. Create test/scripts/python-metrics.test.ts following the python-logging.test.ts pattern:
   - Test getPythonMetricsSetup() returns non-empty string
   - Test returned code contains MetricsLogger class definition
   - Test returned code contains create_metrics_logger function
   - Test returned code contains put_metric method
   - Test returned code contains flush method
   - Test returned code contains _aws EMF structure
   - Test returned code contains CloudWatchMetrics array
   - Test returned code contains unit constants (COUNT, MILLISECONDS, etc.)
   - Test module can be imported from lib/scripts (integration test)
  </action>
  <verify>npm run test:code -- --testPathPattern=python-metrics passes all tests</verify>
  <done>getPythonMetricsSetup exported from lib/scripts, 8+ tests passing</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run test:code` passes (no regressions)
- [ ] `npm run test:code -- --testPathPattern=python-metrics` shows all tests passing
- [ ] lib/scripts/python-metrics.ts exists with documented functions
- [ ] lib/scripts/index.ts exports getPythonMetricsSetup
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No TypeScript errors
- EMF JSON structure matches CloudWatch specification
- Module follows established patterns from python-logging.ts
</success_criteria>

<output>
After completion, create `.planning/phases/10-cloudwatch-metrics/10-01-SUMMARY.md`
</output>
